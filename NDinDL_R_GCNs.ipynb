{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asMhF2nA-FeE"
   },
   "source": [
    "# New developments in deep learning - R-GCNs\n",
    "Github repository: https://github.com/phucdev/NDinDL\n",
    "\n",
    "Isotropic Graph Neural Networks \n",
    "- Representations are learned via differentiable message passing scheme \n",
    "-  All neighbors are treated as equally important \n",
    "-  Starting points: \n",
    "  -  Kipf & Welling: “Semi-Supervised Classification with Graph Convolutional Networks” (https://arxiv.org/abs/1609.02907) \n",
    "  -  Schlichtkrull et al.: “Modeling Relational Data with Graph Convolutional Networks” (https://arxiv.org/abs/1703.06103) \n",
    "- Task: \n",
    "  - Implement Relational Graph convolutional Neural Network for Node Classification\n",
    "\n",
    "Blog posts:\n",
    "- https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780\n",
    "- http://tkipf.github.io/graph-convolutional-networks/\n",
    "\n",
    "Jupyter notebook tutorial:\n",
    "- https://github.com/TobiasSkovgaardJepsen/posts/blob/master/HowToDoDeepLearningOnGraphsWithGraphConvolutionalNetworks/Part2_SemiSupervisedLearningWithSpectralGraphConvolutions/notebook.ipynb\n",
    "\n",
    "Keras implementation:\n",
    "- https://github.com/tkipf/relational-gcn\n",
    "\n",
    "PyTorch implementations\n",
    "- https://github.com/tkipf/pygcn \n",
    "- https://github.com/masakicktashiro/rgcn_pytorch_implementation\n",
    "- https://github.com/mjDelta/relation-gcn-pytorch\n",
    "- https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/4_rgcn.html\n",
    "- https://github.com/rusty1s/pytorch_geometric/blob/master/examples/rgcn.py\n",
    "\n",
    "\n",
    "Datasets directly available via `torch.geometric.datasets.Entities: AIFB, MUTAG\n",
    "- Overview: https://www.uni-mannheim.de/dws/research/resources/sw4ml-benchmark/\n",
    "- Download: http://data.dws.informatik.uni-mannheim.de/rmlod/LOD_ML_Datasets/data/datasets/RDF_Datasets/\n",
    "\n",
    "```tex\n",
    "@InProceedings{10.1007/978-3-319-46547-0_20,\n",
    "  author=\"Ristoski, Petar\n",
    "  and de Vries, Gerben Klaas Dirk\n",
    "  and Paulheim, Heiko\",\n",
    "  editor=\"Groth, Paul\n",
    "  and Simperl, Elena\n",
    "  and Gray, Alasdair\n",
    "  and Sabou, Marta\n",
    "  and Kr{\\\"o}tzsch, Markus\n",
    "  and Lecue, Freddy\n",
    "  and Fl{\\\"o}ck, Fabian\n",
    "  and Gil, Yolanda\",\n",
    "  title=\"A Collection of Benchmark Datasets for Systematic Evaluations of Machine Learning on the Semantic Web\",\n",
    "  booktitle=\"The Semantic Web -- ISWC 2016\",\n",
    "  year=\"2016\",\n",
    "  publisher=\"Springer International Publishing\",\n",
    "  address=\"Cham\",\n",
    "  pages=\"186--194\",\n",
    "  abstract=\"In the recent years, several approaches for machine learning on the Semantic Web have been proposed. However, no extensive comparisons between those approaches have been undertaken, in particular due to a lack of publicly available, acknowledged benchmark datasets. In this paper, we present a collection of 22 benchmark datasets of different sizes. Such a collection of datasets can be used to conduct quantitative performance testing and systematic comparisons of approaches.\",\n",
    "  isbn=\"978-3-319-46547-0\"\n",
    "}\n",
    "```\n",
    "\n",
    "Dataset information:\n",
    "- The AIFB dataset describes the AIFB research institute in terms of its staff, research group, and publications. In the original paper the dataset was first used to predict the affiliation (i.e., research group) for people in the dataset. The dataset contains 178 members of a research group, however the smallest group contains only 4 people, which is removed from the dataset, leaving 4 classes. Also, we remove the employs relation, which is the inverse of the affiliation relation.\n",
    "(176 labeled instances, 4 classes, 8k entities, 45 relations, 28k edges)\n",
    "- The MUTAG dataset is distributed as an example dataset for the DL-Learner toolkit29. It contains information about complex molecules that are potentially carcinogenic, which is given by the `isMutagenic` property. MUTAG is a dataset of molecular graphs, which was later converted to RDF format, where relations either indicate atomic bonds or merely the presence of a certain feature. Labeled entities in MUTAG are only connected via high-degree hub nodes that encode a certain feature. (340 instances, 2 classes, 23k entities, 23 relations, 74k edges)\n",
    "\n",
    "Entity classification results (accuracy averaged over 10 runs) reported in the R-GCN paper:\n",
    "- AIFB: 95.83\n",
    "- MUTAG: 73.23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqY7yxkK_GR3"
   },
   "source": [
    "## Relational GCNs\n",
    "Extension of GCNs: Use a set of relation-specific weight matrices $W_r^{(l)}$, where $r \\in R$ denotes the relation type\n",
    "\n",
    "Propagation model:\n",
    "> $h_i^{l+1} = \\sigma\\left(\\sum_{r\\in R}\\sum_{j\\in N^r_i}\\frac{1}{c_{i,r}}W_r^{(l)}h_j^{(l)}+\\underbrace{W_0^{(l)}h_i^{(l)}}_{\\text{self-connection}}\\right)$\n",
    "\n",
    "where \n",
    "- $N^r_i$ denotes the set of neighbor indices of node $i$ under relation $r \\in R$, \n",
    "- $c_{i,r}$ is a problem-specific normalization constant that can either be learned or chosen in advance (such as $c_{i,r} = |N_i^r|$).\n",
    "\n",
    "Neural network layer update: evaluate message passing update in parallel for every node $i \\in V$.\n",
    "\n",
    "Parameter sharing for highly- multi-relational data: basis decomposition of relation-specific weight matrices\n",
    "> $W_r^{(l)} = \\sum^B_{b=1}a^{(l)}_{r,b}V_b^{(l)}$\n",
    "\n",
    "Linear combination of basis transformations $V_b^{(l)} \\in \\mathbb{R}^{d^{(l+1)}\\times d^{(l)}}$ with learnable coefficients $a^{(l)}_{r,b}$ such that only the coefficients depend on $r$. $B$, the number of basis functions, is a hyperparameter.\n",
    "\n",
    "For entity classification as described in the paper minimize:\n",
    "> $L = -\\sum_{i\\in Y}\\sum^K_{k=1}t_{i,k}\\ln h_{i,k}^{(l)}$\n",
    "\n",
    "whre:\n",
    "- $Y$ is the set of node indices with labels\n",
    "- $K$ is the number of classes (?)\n",
    "- $t_{i,k}$ is the ground-truth label\n",
    "- $h_{i,k}^{(l)}$ is the $k$-th entry of network ouput for $i$-th labeled node\n",
    "\n",
    "# Training and evaluation\n",
    "- 2 layer model with 16 hidden units (dimension of hidden node representation)\n",
    "- 50 epochs with learning rate 0.01 using Adam optimizer\n",
    "- normalization constant $c_{i,r} = |N_i^r|$, i.e. average all incoming messages from a particular relation type\n",
    "- $l2$ penalty on first layer weights $C_{l2} \\in \\{0, 5\\cdot 10^{-4}\\}$\n",
    "- number of basis functions $B \\in \\{0, 10, 20, 30, 40\\}$\n",
    "\n",
    "Results reported\n",
    "- Accuracy and standard error over 10 runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRIGn4OrF22_"
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Deoj0QtlHTTI"
   },
   "source": [
    "### Imports\n",
    "We mainly use pytorch geometric to load the datasets, numpy and scipy to process the data and pytorch for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kbz1HvLsHpA4"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Entities\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy import stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KzEQQhVP2mG"
   },
   "source": [
    "### R-GCN layer\n",
    "This part is inspired by the keras implementation from Thomas Kipf and pytorch based implementations:\n",
    "- https://github.com/tkipf/pygcn\n",
    "- https://github.com/masakicktashiro/rgcn_pytorch_implementation\n",
    "- https://github.com/mjDelta/relation-gcn-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkXVOtDbPrxq"
   },
   "outputs": [],
   "source": [
    "class RGCNConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 num_rels,\n",
    "                 num_bases=-1,\n",
    "                 bias=False,\n",
    "                 activation=None,\n",
    "                 dropout=0.5,\n",
    "                 is_output_layer=False):\n",
    "        r\"\"\"The relational graph convolutional operator from the `\"Modeling\n",
    "        Relational Data with Graph Convolutional Networks\"\n",
    "        <https://arxiv.org/abs/1703.06103>`_ paper\n",
    "\n",
    "        Propagation model:\n",
    "        (1) $h_i^{l+1} = \\sigma\\left(\\sum_{r\\in R}\\sum_{j\\in N^r_i}\\frac{1}{c_{i,r}}W_r^{(l)}h_j^{(l)}+\n",
    "        \\underbrace{W_0^{(l)}h_i^{(l)}}_{\\text{self-connection}}\\right)$\n",
    "\n",
    "        where\n",
    "        - $N^r_i$ denotes the set of neighbor indices of node $i$ under relation $r \\in R$,\n",
    "        - $c_{i,r}$ is a problem-specific normalization constant that can either be learned or chosen in advance\n",
    "          (such as $c_{i,r} = |N_i^r|$).\n",
    "\n",
    "        Neural network layer update: evaluate message passing update in parallel for every node $i \\in V$.\n",
    "\n",
    "        Parameter sharing for highly- multi-relational data: basis decomposition of relation-specific weight matrices\n",
    "        (2) $W_r^{(l)} = \\sum^B_{b=1}a^{(l)}_{r,b}V_b^{(l)}$\n",
    "\n",
    "        Linear combination of basis transformations $V_b^{(l)} \\in \\mathbb{R}^{d^{(l+1)}\\times d^{(l)}}$ with learnable\n",
    "        coefficients $a^{(l)}_{r,b}$ such that only the coefficients depend on $r$. $B$, the number of basis functions,\n",
    "        is a hyperparameter.\n",
    "\n",
    "        :param input_dim: Input dimension\n",
    "        :param output_dim: Output dimension\n",
    "        :param num_rels: Number of relation types\n",
    "        :param num_bases: Number of bases used in basis decomposition of relation-specific weight matrices\n",
    "        :param bias: Optional additive bias\n",
    "        :param activation: Activation function\n",
    "        :param dropout: Dropout\n",
    "        :param is_output_layer: Indicates whether this layer is the output layer\n",
    "        \"\"\"\n",
    "        super(RGCNConv, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.is_output_layer = is_output_layer\n",
    "\n",
    "        # Number of bases for the basis decomposition can be less or equal to \n",
    "        # the number of relation types\n",
    "        if self.num_bases <= 0 or self.num_bases > self.num_rels:\n",
    "            self.num_bases = self.num_rels\n",
    "\n",
    "        # Weight bases in equation (2)\n",
    "        # V_b if self.num_bases < self.num_rels, \n",
    "        # W_r if self.num_bases == self.num_rels\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.Tensor(self.num_bases * self.input_dim, self.output_dim))\n",
    "\n",
    "        # Use basis decomposition otherwise if num_bases = num_rels we can just \n",
    "        # use one weight matrix per relation type\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # linear combination coefficients a^{(l)}_{r, b} in equation (2)\n",
    "            self.w_comp = nn.Parameter(\n",
    "                torch.Tensor(self.num_rels, self.num_bases))\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = nn.Parameter(torch.Tensor(self.output_dim))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize trainable parameters, see following link for explanation:\n",
    "        # https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79\n",
    "        # Xavier initialization: improved weight initialization method enabling \n",
    "        # quicker convergence and higher accuracy\n",
    "        # gain is an optional scaling factor, here we use the recommended gain \n",
    "        # value for the given nonlinearity function\n",
    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        if self.num_bases < self.num_rels:\n",
    "            nn.init.xavier_uniform_(self.w_comp, gain=nn.init.calculate_gain('relu'))\n",
    "        if self.bias:\n",
    "            nn.init.xavier_uniform_(self.b, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        supports = []\n",
    "        num_nodes = adj_t[0].shape[0]\n",
    "        for i, adj in enumerate(adj_t):\n",
    "            if x is not None:\n",
    "                supports.append(torch.spmm(adj, x))\n",
    "            else:\n",
    "                supports.append(adj)\n",
    "        supports = torch.cat(supports, dim=1)   # (num_rel, num_nodes*num_rel)\n",
    "\n",
    "        # Calculate relation specific weight matrices\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # Generate all weights from bases as in equation (2)\n",
    "            weight = self.weight.reshape(self.num_bases, self.input_dim, self.output_dim).permute(1, 0, 2)\n",
    "\n",
    "            # Matrix product: learnable coefficients a_{r, b} and basis transformations V_b\n",
    "            # (self.num_rels, self.num_bases) x (self.input_dim, self.num_bases, self.output_dim)\n",
    "            weight = torch.matmul(self.w_comp, weight)  # (self.input_dim, self.num_rels, self.output_dim)\n",
    "            weight = weight.reshape(self.input_dim * self.num_rels, self.output_dim)\n",
    "        else:\n",
    "            weight = self.weight\n",
    "\n",
    "        out = torch.spmm(supports, weight)  # (num_nodes, num_rels)\n",
    "\n",
    "        # If x is None add dropout to output, by elementwise multiplying with \n",
    "        # column vector of ones, with dropout applied to the vector of ones.\n",
    "        if x is None:\n",
    "            temp = torch.ones(num_nodes).to(out.device)\n",
    "            temp_drop = F.dropout(temp, self.dropout)\n",
    "            out = (out.transpose(1, 0) * temp_drop).transpose(1, 0)\n",
    "\n",
    "        if self.bias:\n",
    "            out += self.b\n",
    "\n",
    "        out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYxn57MtP6E8"
   },
   "source": [
    "### R-GCN model\n",
    "This part is somewhat inspired by: https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/4_rgcn.html.\n",
    "We borrowed the idea of building the model using separate build functions for the input, hidden and output layer in order to allow building models with multiple hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxNhsYh2P71e"
   },
   "outputs": [],
   "source": [
    "class RGCN(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels,\n",
    "                 num_bases=-1, num_hidden_layers=1, dropout=0.5, bias=False):\n",
    "        \"\"\"\n",
    "        Implementation of R-GCN from the `\"Modeling\n",
    "        Relational Data with Graph Convolutional Networks\"\n",
    "        <https://arxiv.org/abs/1703.06103>`_ paper\n",
    "\n",
    "        :param num_nodes: Number of nodes (input dimension)\n",
    "        :param h_dim: Hidden dimension\n",
    "        :param out_dim: Output dimension\n",
    "        :param num_rels: Number of relation types\n",
    "        :param num_bases: Number of basis functions\n",
    "        :param num_hidden_layers: Number of hidden layers\n",
    "        :param dropout: Dropout probability\n",
    "        :param bias: Whether to use an additive bias\n",
    "        \"\"\"\n",
    "        super(RGCN, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # input to hidden\n",
    "        i2h = self.build_input_layer()\n",
    "        self.layers.append(i2h)\n",
    "        # hidden to hidden\n",
    "        for _ in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer()\n",
    "            self.layers.append(h2h)\n",
    "        # hidden to output\n",
    "        h2o = self.build_output_layer()\n",
    "        self.layers.append(h2o)\n",
    "\n",
    "    def build_input_layer(self):\n",
    "        return RGCNConv(self.num_nodes, self.h_dim, self.num_rels, self.num_bases, activation=F.relu,\n",
    "                        dropout=self.dropout, bias=self.bias)\n",
    "\n",
    "    def build_hidden_layer(self):\n",
    "        return RGCNConv(self.h_dim, self.h_dim, self.num_rels, self.num_bases, activation=F.relu,\n",
    "                        dropout=self.dropout, bias=self.bias)\n",
    "\n",
    "    def build_output_layer(self):\n",
    "        return RGCNConv(self.h_dim, self.out_dim, self.num_rels, self.num_bases, activation=partial(F.softmax, dim=-1),\n",
    "                        dropout=self.dropout, is_output_layer=True, bias=self.bias)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.layers:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, adj_t)\n",
    "            if not layer.is_output_layer:\n",
    "                out = F.dropout(out, self.dropout, self.training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiJrPM6SRjZb"
   },
   "source": [
    "### Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWb3JIQeRoE3"
   },
   "outputs": [],
   "source": [
    "def train(model, x, adj_t, optimizer, loss_fn, train_idx, train_y):\n",
    "    model.train()\n",
    "\n",
    "    # Zero grad the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    # Feed the data into the model\n",
    "    out = model(x, adj_t)\n",
    "    # Feed the sliced output and label to loss_fn\n",
    "    labels = torch.LongTensor(train_y).to(out.device)\n",
    "    loss = loss_fn(out[train_idx], labels)\n",
    "\n",
    "    # Backpropagation, optimizer\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, x, adj_t, train_idx, train_y, test_idx, test_y):\n",
    "    model.eval()\n",
    "\n",
    "    # Output of model on all data\n",
    "    out = model(x, adj_t)\n",
    "    # Get predicted class labels\n",
    "    pred = out.argmax(dim=-1).cpu()\n",
    "\n",
    "    # Evaluate prediction accuracy\n",
    "    train_acc = pred[train_idx].eq(train_y).to(torch.float).mean()\n",
    "    test_acc = pred[test_idx].eq(test_y).to(torch.float).mean()\n",
    "    return train_acc.item(), test_acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbGcX2dyO2vx"
   },
   "source": [
    "### Data preprocessing functions\n",
    "In order to use the data from `torch.geometric.datasets.Entities` with our R-GCN implementation we have to convert the dataset and construct the adjacency matrices from the edge index and edge type arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G59p9-N8NDSk"
   },
   "outputs": [],
   "source": [
    "def get_adjacency_matrices(data):\n",
    "    \"\"\"\n",
    "    Converts torch_geometric.datasets.entities data to relation type specific \n",
    "    adjacency matrices\n",
    "    :param data: torch_geometric.datasets.entities data\n",
    "    :return:\n",
    "        A: list of relation type specific adjacency matrices\n",
    "    \"\"\"\n",
    "    num_rels = data.num_rels\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    A = []\n",
    "    source_nodes = data.edge_index[0].numpy()\n",
    "    target_nodes = data.edge_index[1].numpy()\n",
    "\n",
    "    # Get edges for given (relation) edge type and construct adjacency matrix\n",
    "    for i in range(num_rels):\n",
    "        indices = np.argwhere(np.asarray(data.edge_type) == i).squeeze(axis=1)\n",
    "        r_source_nodes = source_nodes[indices]\n",
    "        r_target_nodes = target_nodes[indices]\n",
    "        a = sp.csr_matrix(\n",
    "            (np.ones(len(indices)), (r_source_nodes, r_target_nodes)), \n",
    "            shape=(num_nodes, num_nodes))\n",
    "        A.append(a)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AS_u3qNO9Ls"
   },
   "source": [
    "The following functions are for normalizing the matrices individually and converting them to sparse tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MqiLocPl708"
   },
   "outputs": [],
   "source": [
    "def normalize(adj_matrix):\n",
    "    \"\"\"\n",
    "    Normalizes the adjacency matrix\n",
    "    :param adj_matrix: Adjacency matrix\n",
    "    :return:\n",
    "        out: Normalized adjacency matrix\n",
    "    \"\"\"\n",
    "    node_degrees = np.array(adj_matrix.sum(axis=1)).flatten()\n",
    "    # Essentially 1. / node_degrees, while avoiding division by zero warning\n",
    "    norm_const = np.divide(np.ones_like(node_degrees), node_degrees, out=np.zeros_like(node_degrees),\n",
    "                           where=node_degrees != 0)\n",
    "    D_inv = sp.diags(norm_const)\n",
    "    out = D_inv.dot(adj_matrix).tocsr()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oloGQFaRl79f"
   },
   "outputs": [],
   "source": [
    "def to_sparse_tensor(sparse_array):\n",
    "    \"\"\"\n",
    "    Converts sparse array (normalized adjacency matrix) to sparse tensor\n",
    "    :param sparse_array: Sparse array (normalized adjacency matrix)\n",
    "    :return:\n",
    "        sparse_tensor: Converted sparse tensor\n",
    "    \"\"\"\n",
    "    if len(sp.find(sparse_array)[-1]) > 0:\n",
    "        # Get indices and values of nonzero elements in matrix\n",
    "        v = torch.FloatTensor(sp.find(sparse_array)[-1])\n",
    "        i = torch.LongTensor(sparse_array.nonzero())\n",
    "        shape = sparse_array.shape\n",
    "        sparse_tensor = torch.sparse_coo_tensor(i, v, torch.Size(shape))\n",
    "    else:\n",
    "        sparse_tensor = torch.sparse_coo_tensor(sparse_array.shape[0], sparse_array.shape[1])\n",
    "    return sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQOMGMlHF060"
   },
   "source": [
    "## Experiments\n",
    "In theory this model should work with all 4 datasets. However BGS and AM contain huge graphs, which require lots of memory. We recommend to only use AIFB or MUTAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1-jVEyPISax"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lP7edMgQ90td"
   },
   "source": [
    "Data loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Get6gjDs-G0a"
   },
   "outputs": [],
   "source": [
    "def load_data(dataset_name):\n",
    "  # Load data via pytorch geometric\n",
    "  path = osp.join('.', 'data', 'Entities')\n",
    "  dataset = Entities(path, dataset_name)\n",
    "  data = dataset[0]\n",
    "\n",
    "  data.num_nodes = maybe_num_nodes(data.edge_index)\n",
    "  data.num_rels = dataset.num_relations\n",
    "\n",
    "  # Construct relation type specific adjacency matrices from data.edge_index and data.edge_type in utils\n",
    "  A = get_adjacency_matrices(data)\n",
    "\n",
    "  adj_t = []\n",
    "  # Normalize matrices individually and convert to sparse tensors\n",
    "  for a in A:\n",
    "      nor_a = normalize(a)\n",
    "      if len(nor_a.nonzero()[0]) > 0:\n",
    "          tensor_a = to_sparse_tensor(nor_a)\n",
    "          adj_t.append(tensor_a.to(device))\n",
    "\n",
    "  # Replace if features are available\n",
    "  x = None    \n",
    "  return dataset, data, adj_t, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At5mtJ2l-wWg"
   },
   "source": [
    "Experiment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkneR-x49xoW"
   },
   "outputs": [],
   "source": [
    "def run_experiment(dataset_name, args, runs=10):\n",
    "  dataset, data, adj_t, x = load_data(dataset_name)\n",
    "  # Initialize RGCN model\n",
    "  model = RGCN(\n",
    "      num_nodes=data.num_nodes,\n",
    "      h_dim=args[\"h_dim\"],\n",
    "      out_dim=dataset.num_classes,\n",
    "      num_rels=dataset.num_relations,\n",
    "      num_bases=args[\"num_bases\"],\n",
    "      dropout=args[\"dropout\"]\n",
    "  ).to(device)\n",
    "\n",
    "  test_accs = []\n",
    "\n",
    "  for i in range(1, runs+1):\n",
    "    print('------------------------------------------------')\n",
    "    print(f'Model run {i}')\n",
    "    print('------------------------------------------------')\n",
    "    # Reset the parameters to initial random values\n",
    "    model.reset_parameters()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"], weight_decay=args[\"l2\"])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_test_acc = 0\n",
    "    test_acc = 0\n",
    "    # Train and evaluate model\n",
    "    for epoch in range(1, args[\"epochs\"] + 1):\n",
    "        loss = train(model, x, adj_t, optimizer, loss_fn, data.train_idx, data.train_y)\n",
    "        train_acc, test_acc = test(model, x, adj_t, data.train_idx, data.train_y, data.test_idx, data.test_y)\n",
    "        if test_acc > best_test_acc:\n",
    "          best_test_acc = test_acc\n",
    "        if epoch == 1 or (epoch % 10) == 0:\n",
    "          print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {train_acc:.4f} '\n",
    "                f'Test: {test_acc:.4f}')\n",
    "    test_accs.append(test_acc)  # alternatively use the best test acc\n",
    "    print(f'Best test accuracy: {best_test_acc:.4f}')\n",
    "  \n",
    "  avg_test_acc = np.mean(test_accs)\n",
    "  sem_test_acc = stats.sem(test_accs)\n",
    "\n",
    "  print('------------------------------------------------')\n",
    "  print(f'Average test accuracy over {runs} runs: {100 * avg_test_acc:.2f}+-{100 * sem_test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVUlGs9554Gq"
   },
   "source": [
    "### AIFB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9bfQVmtSERp"
   },
   "outputs": [],
   "source": [
    "# Parameters from the RGCN paper\n",
    "args = {\n",
    "        'h_dim': 16,\n",
    "        'num_bases': -1,\n",
    "        'num_hidden_layers': 0,\n",
    "        'dropout': 0.,\n",
    "        'lr': 0.01,\n",
    "        'l2': 0.,\n",
    "        'bias': False,\n",
    "        'epochs': 50,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASxlpmo1mLZS"
   },
   "source": [
    "Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmjSBoiY_iJk",
    "outputId": "26878ea2-883a-494c-e698-fa65d3a3629b"
   },
   "outputs": [],
   "source": [
    "run_experiment(dataset_name=\"AIFB\", args=args, runs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AAoTFkJ6DEt"
   },
   "source": [
    "### MUTAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJ2dCWKVloI6"
   },
   "outputs": [],
   "source": [
    "dataset_name = \"MUTAG\"  # choices=['AIFB', 'MUTAG', 'BGS', 'AM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzPoh2x46Vnz"
   },
   "outputs": [],
   "source": [
    "# Parameters from the RGCN paper\n",
    "args = {\n",
    "        'h_dim': 16,\n",
    "        'num_bases': 30,\n",
    "        'num_hidden_layers': 0,\n",
    "        'dropout': 0.,\n",
    "        'lr': 0.01,\n",
    "        'l2': 0.0005,\n",
    "        'bias': False,\n",
    "        'epochs': 50,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzxkK3WQ_fen"
   },
   "source": [
    "Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOeRNv3y84Vu",
    "outputId": "07d02396-3da6-4ee8-8c41-88b42153fcd4"
   },
   "outputs": [],
   "source": [
    "run_experiment(dataset_name=\"MUTAG\", args=args, runs=10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NDinDL_R-GCNs.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
