{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# New Developments in Deep Learning: Isotropic Graph Neural Networks\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "Graphs are used to model and analyze a variety of phenomena – be it social networks, protein-protein interactions or citation networks. These are rather irregular in comparison to structures like sequences or grids, each of them representable as a graph as well. Popular Machine Learning models have been tailored to these regular forms of graphs, specifically Recurrent Neural Networks (RNNs) for sequences (e.g. sentences, time series) and Convolutional Neural Networks (CNNs) for grids (e.g. images). To be able to deal with general graphs however, Graph Neural Networks (GNNs) were devised. GNNs base upon a differentiable message passing scheme, i.e. vector messages are passed between nodes and updated using neural networks (Hamilton, 2020). In this project, the focus is on Graph Convolutional Networks (GCNs), but before going into detail, we will first define some important terms and illustrate potential use cases as well as previous approaches."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Background\n",
    "\n",
    "A graph is commonly defined as a tuple $G = (V, E)$ where $V$ denotes the set of vertices and $E \\subseteq V \\times V$ is the set of edges between vertices. The edge relation is typically represented as an adjacency matrix $M$ where $M_{i,j} = 1$ iff $(i,j) \\in E$. One or more features may be assigned to the vertices and/or edges in a graph. Nodes may further be encoded in the form of embeddings, i.e., as \"low-dimensional vectors that summarize their graph position and the structure in their local neighborhood\" (Hamilton, 2020).\n",
    "\n",
    "Common tasks related to graphs are node classification, link prediction, graph classification and community detection (Hamilton, 2020). In this seminar project, we will focus on node classification using Relational Graph Convolutional Networks (R-GCN). Node classification refers to predicting the labels for a set of unlabelled nodes in a graph, given a (usually rather small) set of labelled nodes of the same graph. \\\\\n",
    "Originally, this was achieved by assigning some kind of node-level statistics like the node degree to each node and using these as input features to a standard machine learning classifier. Another approach consists in *encoding* nodes as so-classed shallow embeddings, which are computed for each node uniquely, and then reconstructing graph characteristics, like an edge between two nodes, from these embeddings (corresponding to a *decoding* step, e.g. based on random walks). \\\\\n",
    "However, shallow embeddings have some major shortcomings: they do not allow for parameter sharing, which would improve the computational efficiency and constitute a form of regularization. They further do not incorporate the graph structure or feature information, which could improve the quality of the embeddings. Lastly, they cannot generalize to nodes unseen during the training. Graph Neural Networks (GNNs) tackle these deficiencies, and thus enable the computation of more sophisticated embeddings that can be used to classify nodes or predict links (Hamilton, 2020). In the following, we are going to elaborate on (Relational) Graph Convolutional Networks in order to understand their applicability to the problem of node classification."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Graph Convolutional Networks (GCNs)\n",
    "Kipf & Welling (2016) proposed Graph Convolutional Networks (GCNs) that make use of a first-order approximation of spectral graph convolutions. In essence, that corresponds to the aforementioned message passing mechanism enhanced with symmetric normalization, i.e. the representation of a node $i$ is computed by aggregating and normalizing the information held by its neighbors $N_i$ and combining the result with $i$'s previous representation. Accordingly, a GCN layer consists of normalized message passing operations, followed by a non-linear activation. This idea is condensed in the following formula which represents the update of node $i$ in layer $l+1$:\n",
    "> $h_i^{l+1} = \\sigma\\left(\\sum_{j\\in N_i}{c_{i,j}}W_i^{(l)^T}h_j^{(l)}+\\underbrace{W_0^{(l)^T}h_i^{(l)}}_{\\text{self-connection}}\\right)$\n",
    "\n",
    "In detail, a hidden state $v_i$ of a node $i$ in layer $l+1$ is computed by\n",
    "1.\tAggregating the hidden states $v_j$ of the neighbouring nodes $j \\in N_i$ by summing up the products of each hidden state $v_j$ with\n",
    "  - a learnable $d_l \\times d_{l+1}$ parameter matrix $W_1^{(l)}$ and\n",
    "  - normalization constant $c_{i,j}=\\frac{1}{\\sqrt{D_{i,i}D_{j,j}}}$ where $D_{i,i}$ is the node degree of node $i$. The normalization constant is particularly important, since it reduces the impact of high node degrees.\n",
    "2.\tApplying another $d_l \\times d_{l+1}$ parameter matrix $W_0^{(l)}$ to the hidden state $v_i$ of node $i$ in layer $l$ (which corresponds to adding a self-loop) and adding that to the overall aggregated value\n",
    "3.\tApplying an element-wise non-linear activation function $\\sigma$ like ReLu to the overall result\n",
    "4.\tAssigning the result to node $v_i$ as its hidden state in layer $l+1$\n",
    "\n",
    "To construct a $n$-layer GCN, these steps are repeated $n$ times for each node in parallel by using the output of the $i$-th layer as the input of the $i+1$-th layer. The original node features serve as input to the first layer, and the output of the last layer is simultaneously the output of the overall GCN. Stacking the layers in that way captures dependencies across several relational steps, i.e. after k layers, a node embedding incorporates information, both structural and feature-based, of its k-hop neighbourhood (Hamilton, 2020). These so-called vanilla GCNs are an example of *Isotropic* Graph Neural Networks, i.e. when calculating the hidden state for each node, each of its neighbors is included with equal weight (Dwivedi et al., 2020)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Relational Graph Convolutional Networks (R-GCNs)\n",
    "In this seminar, we employed an advancement of GCNs, namely Relational GCNs (R-GCNs) that was proposed by Schlichtkrull et al. (2018). R-GCNs support multi-relational data, i.e. graphs with a type assigned to each edge. A common example of such data are knowledge graphs (like DBPedia or Wikidata), which may be conceived as a collection of triples *(subject, predicate, object)*. Subject and object correspond to typed entities and the predicate denotes the relation type. A proper representation of knowledge graphs are directed, labeled multigraphs with entities as nodes and triples determining the edge relation.\\\n",
    "To support this kind of data, the previously presented computation of a layer is enhanced by using a separate weight matrix $W_r^{(l)}$ for each relation type $r \\in R$. Broadly speaking, the hidden states of neighbouring nodes are separately transformed for each relation type and direction of an edge. The node in question is also transformed separately (corresponding to a self-loop). These transformations are then aggregated in a normalized manner and passed through an activation function. This computation is individually performed for each node in the graph. The following formula comprises the node update for a node $i$ in layer $l+1$:\n",
    "\n",
    "> $h_i^{l+1} = \\sigma\\left(\\sum_{r\\in R}\\sum_{j\\in N^r_i}\\frac{1}{c_{i,r}}W_r^{(l)}h_j^{(l)}+\\underbrace{W_0^{(l)}h_i^{(l)}}_{\\text{self-connection}}\\right)$\n",
    "\n",
    "where\n",
    "- $N^r_i$ denotes the set of neighbors of node $i$ specific to the relation $r \\in R$\n",
    "- $c_{i,r}$ is a normalization constant that can either be learned or set by hand (e.g. $c_{i,r} = |N_i^r|$).\n",
    "\n",
    "#### Regularization\n",
    "In the corresponding work, Schlichtkrull et al. (2018) further present regularization techniques in the form of basis decomposition and block-diagonal decomposition. Regularization of the weights in R-GCN layers is needed due to the increasing number of parameters with the number of relations in the graph which could lead to overfitting and large models. For that reason, Schlichtkrull et al. leveraged basis decomposition of the relation-specific weight matrices as follows:\n",
    "\n",
    "> $W_r^{(l)} = \\sum^B_{b=1}a^{(l)}_{r,b}V_b^{(l)}$\n",
    "\n",
    "The relation-specific matrices are thus represented as linear combinations of basis transformations $V_b^{(l)} \\in \\mathbb{R}^{d^{(l+1)}\\times d^{(l)}}$ with learnable coefficients $a^{(l)}_{r,b}$ such that only the coefficients have to be adapted for each $r \\in R$ and not the complete weight matrix as previously . $B$, the number of basis functions, is a hyperparameter. The basis decomposition acts as a weight sharing mechanism between different relation types, such that the number of parameters that need to be learned is reduced. Schlichtkrull et al. further assume that basis decomposition can avert the risk of overfitting on rare relations due to the parameter sharing between rare and frequent relations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Entity Classification with R-GCNs\n",
    "\n",
    "In order to perform entity classification, softmax classifiers are applied to the output of the R-GCN, i.e., to the final embedding of each node. That way, labels are predicted. During the training of the model, the cross-entropy loss $L$ is minimized on the set of *labelled* nodes as follows:\n",
    "> $L = -\\sum_{i\\in Y}\\sum^K_{k=1}t_{i,k}\\ln h_{i,k}^{(l)}$\n",
    "\n",
    "where:\n",
    "- $Y$ is the set of node indices with labels\n",
    "- $K$ is the number of classes\n",
    "- $t_{i,k}$ is the ground-truth label\n",
    "- $h_{i,k}^{(l)}$ is the $k$-th entry of network ouput for $i$-th labeled node\n",
    "\n",
    "In practice, the training is realized by use of (full-batch) gradient descent (Schlichtkrull, 2018).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementation\n",
    "Next, we will walk you through our implementation which encompasses the preparatory setup as well as the implementation of the R-GCN.\n",
    "Github repository: https://github.com/phucdev/NDinDL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare the colab environment\n",
    "We need to check the python and cuda version in the colab environment and adjust the installation of the requirements accordingly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python --version"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Installing requirements"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu111.html\n",
    "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu111.html\n",
    "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.8.0+cu111.html\n",
    "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.8.0+cu111.html\n",
    "!pip install torch-geometric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports\n",
    "We mainly use pytorch geometric to load the datasets, numpy and scipy to process the data and pytorch for the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Entities\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy import stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import os.path as osp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### R-GCN layer\n",
    "This part is inspired by the keras implementation from Thomas Kipf and pytorch based implementations:\n",
    "- https://github.com/tkipf/pygcn\n",
    "- https://github.com/masakicktashiro/rgcn_pytorch_implementation\n",
    "- https://github.com/mjDelta/relation-gcn-pytorch\n",
    "\n",
    "During the initialization of an R-GCN layer, everything is set up for basis decomposition. To achieve quicker convergence and higher accuracy, we employ Xavier initialization to initialize the trainable parameters (Dellinger, 2019). We use the recommmended gain value for the given non-linearity function (here: ReLu), which is an optional scaling factor. We also give the user the opportunity to define a dropout probability, although we do not use it in the experiments. Dropout is yet another regularization technique that essentially refers to randomly dropping units during the training of a neural network (Srivastava, 2014). That way, overfitting can be mitigated."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RGCNConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 num_rels,\n",
    "                 num_bases=-1,\n",
    "                 bias=False,\n",
    "                 activation=None,\n",
    "                 dropout=0.5,\n",
    "                 is_output_layer=False):\n",
    "        r\"\"\"The relational graph convolutional operator from the `\"Modeling\n",
    "        Relational Data with Graph Convolutional Networks\"\n",
    "        <https://arxiv.org/abs/1703.06103>`_ paper\n",
    "\n",
    "        Propagation model:\n",
    "        (1) $h_i^{l+1} = \\sigma\\left(\\sum_{r\\in R}\\sum_{j\\in N^r_i}\\frac{1}{c_{i,r}}W_r^{(l)}h_j^{(l)}+\n",
    "        \\underbrace{W_0^{(l)}h_i^{(l)}}_{\\text{self-connection}}\\right)$\n",
    "\n",
    "        where\n",
    "        - $N^r_i$ denotes the set of neighbor indices of node $i$ under relation $r \\in R$,\n",
    "        - $c_{i,r}$ is a problem-specific normalization constant that can either be learned or chosen in advance\n",
    "          (such as $c_{i,r} = |N_i^r|$).\n",
    "\n",
    "        Neural network layer update: evaluate message passing update in parallel for every node $i \\in V$.\n",
    "\n",
    "        Parameter sharing for highly- multi-relational data: basis decomposition of relation-specific weight matrices\n",
    "        (2) $W_r^{(l)} = \\sum^B_{b=1}a^{(l)}_{r,b}V_b^{(l)}$\n",
    "\n",
    "        Linear combination of basis transformations $V_b^{(l)} \\in \\mathbb{R}^{d^{(l+1)}\\times d^{(l)}}$ with learnable\n",
    "        coefficients $a^{(l)}_{r,b}$ such that only the coefficients depend on $r$. $B$, the number of basis functions,\n",
    "        is a hyperparameter.\n",
    "\n",
    "        :param input_dim: Input dimension\n",
    "        :param output_dim: Output dimension\n",
    "        :param num_rels: Number of relation types\n",
    "        :param num_bases: Number of bases used in basis decomposition of relation-specific weight matrices\n",
    "        :param bias: Optional additive bias\n",
    "        :param activation: Activation function\n",
    "        :param dropout: Dropout\n",
    "        :param is_output_layer: Indicates whether this layer is the output layer\n",
    "        \"\"\"\n",
    "        super(RGCNConv, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.is_output_layer = is_output_layer\n",
    "\n",
    "        # Number of bases for the basis decomposition can be less or equal to\n",
    "        # the number of relation types\n",
    "        if self.num_bases <= 0 or self.num_bases > self.num_rels:\n",
    "            self.num_bases = self.num_rels\n",
    "\n",
    "        # Weight bases in equation (2)\n",
    "        # V_b if self.num_bases < self.num_rels,\n",
    "        # W_r if self.num_bases == self.num_rels\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.Tensor(self.num_bases * self.input_dim, self.output_dim))\n",
    "\n",
    "        # Use basis decomposition otherwise if num_bases = num_rels we can just\n",
    "        # use one weight matrix per relation type\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # linear combination coefficients a^{(l)}_{r, b} in equation (2)\n",
    "            self.w_comp = nn.Parameter(\n",
    "                torch.Tensor(self.num_rels, self.num_bases))\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = nn.Parameter(torch.Tensor(self.output_dim))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize trainable parameters, see following link for explanation:\n",
    "        # https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79\n",
    "        # Xavier initialization: improved weight initialization method enabling\n",
    "        # quicker convergence and higher accuracy\n",
    "        # gain is an optional scaling factor, here we use the recommended gain\n",
    "        # value for the given nonlinearity function\n",
    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        if self.num_bases < self.num_rels:\n",
    "            nn.init.xavier_uniform_(self.w_comp, gain=nn.init.calculate_gain('relu'))\n",
    "        if self.bias:\n",
    "            nn.init.xavier_uniform_(self.b, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        supports = []\n",
    "        num_nodes = adj_t[0].shape[0]\n",
    "        for i, adj in enumerate(adj_t):\n",
    "            if x is not None:\n",
    "                supports.append(torch.spmm(adj, x))\n",
    "            else:\n",
    "                supports.append(adj)\n",
    "        supports = torch.cat(supports, dim=1)   # (num_rel, num_nodes*num_rel)\n",
    "\n",
    "        # Calculate relation specific weight matrices\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # Generate all weights from bases as in equation (2)\n",
    "            weight = self.weight.reshape(self.num_bases, self.input_dim, self.output_dim).permute(1, 0, 2)\n",
    "\n",
    "            # Matrix product: learnable coefficients a_{r, b} and basis transformations V_b\n",
    "            # (self.num_rels, self.num_bases) x (self.input_dim, self.num_bases, self.output_dim)\n",
    "            weight = torch.matmul(self.w_comp, weight)  # (self.input_dim, self.num_rels, self.output_dim)\n",
    "            weight = weight.reshape(self.input_dim * self.num_rels, self.output_dim)\n",
    "        else:\n",
    "            weight = self.weight\n",
    "\n",
    "        out = torch.spmm(supports, weight)  # (num_nodes, num_rels)\n",
    "\n",
    "        # If x is None add dropout to output, by elementwise multiplying with\n",
    "        # column vector of ones, with dropout applied to the vector of ones.\n",
    "        if x is None:\n",
    "            temp = torch.ones(num_nodes).to(out.device)\n",
    "            temp_drop = F.dropout(temp, self.dropout)\n",
    "            out = (out.transpose(1, 0) * temp_drop).transpose(1, 0)\n",
    "\n",
    "        if self.bias:\n",
    "            out += self.b\n",
    "\n",
    "        out = self.activation(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `forward` function is basically the place where we define how the output of a layer is computed. First, we compute the relation-specific matrices based on the bases if necessary. Then we compute the product of the tensor representing the relation-specific, normalized adjacency matrices (which may also incorporate the features if any are given) and the weight matrices. Lastly, we also factor the dropout and the bias in, before applying the activation function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### R-GCN model\n",
    "This part is somewhat inspired by: https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/4_rgcn.html.\n",
    "We borrowed the idea of building the model using separate build functions for the input, hidden and output layer in order to allow building models with multiple hidden layers. That way, we can easily stack the layers (implemented as `RGCNConv`) as it was described in the introductory section."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RGCN(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels,\n",
    "                 num_bases=-1, num_hidden_layers=1, dropout=0.5, bias=False):\n",
    "        \"\"\"\n",
    "        Implementation of R-GCN from the `\"Modeling\n",
    "        Relational Data with Graph Convolutional Networks\"\n",
    "        <https://arxiv.org/abs/1703.06103>`_ paper\n",
    "\n",
    "        :param num_nodes: Number of nodes (input dimension)\n",
    "        :param h_dim: Hidden dimension\n",
    "        :param out_dim: Output dimension\n",
    "        :param num_rels: Number of relation types\n",
    "        :param num_bases: Number of basis functions\n",
    "        :param num_hidden_layers: Number of hidden layers\n",
    "        :param dropout: Dropout probability\n",
    "        :param bias: Whether to use an additive bias\n",
    "        \"\"\"\n",
    "        super(RGCN, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # input to hidden\n",
    "        i2h = self.build_input_layer()\n",
    "        self.layers.append(i2h)\n",
    "        # hidden to hidden\n",
    "        for _ in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer()\n",
    "            self.layers.append(h2h)\n",
    "        # hidden to output\n",
    "        h2o = self.build_output_layer()\n",
    "        self.layers.append(h2o)\n",
    "\n",
    "    def build_input_layer(self):\n",
    "        return RGCNConv(self.num_nodes, self.h_dim, self.num_rels, self.num_bases, activation=F.relu,\n",
    "                        dropout=self.dropout, bias=self.bias)\n",
    "\n",
    "    def build_hidden_layer(self):\n",
    "        return RGCNConv(self.h_dim, self.h_dim, self.num_rels, self.num_bases, activation=F.relu,\n",
    "                        dropout=self.dropout, bias=self.bias)\n",
    "\n",
    "    def build_output_layer(self):\n",
    "        return RGCNConv(self.h_dim, self.out_dim, self.num_rels, self.num_bases, activation=partial(F.softmax, dim=-1),\n",
    "                        dropout=self.dropout, is_output_layer=True, bias=self.bias)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.layers:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, adj_t)\n",
    "            if not layer.is_output_layer:\n",
    "                out = F.dropout(out, self.dropout, self.training)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training and Evaluation Functions\n",
    "\n",
    "The training step consists of calculating the loss for the labelled (training) nodes and propagating it backwards through the network. Lastly, the weights are adapted in `optimizer.step()`. The `test` function enables us to calculate the accuracy on the test as well as the training data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model, x, adj_t, optimizer, loss_fn, train_idx, train_y):\n",
    "    model.train()\n",
    "\n",
    "    # Zero grad the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    # Feed the data into the model\n",
    "    out = model(x, adj_t)\n",
    "    # Feed the sliced output and label to loss_fn\n",
    "    labels = torch.LongTensor(train_y).to(out.device)\n",
    "    loss = loss_fn(out[train_idx], labels)\n",
    "\n",
    "    # Backpropagation, optimizer\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, x, adj_t, train_idx, train_y, test_idx, test_y):\n",
    "    model.eval()\n",
    "\n",
    "    # Output of model on all data\n",
    "    out = model(x, adj_t)\n",
    "    # Get predicted class labels\n",
    "    pred = out.argmax(dim=-1).cpu()\n",
    "\n",
    "    # Evaluate prediction accuracy\n",
    "    train_acc = pred[train_idx].eq(train_y).to(torch.float).mean()\n",
    "    test_acc = pred[test_idx].eq(test_y).to(torch.float).mean()\n",
    "    return train_acc.item(), test_acc.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data preprocessing functions\n",
    "In order to use the data from `torch.geometric.datasets.Entities` with our R-GCN implementation, we have to convert the dataset and construct the adjacency matrices from the edge index and edge type arrays. We therefore compute a collection of adjacency matrices, with one adjacency matrix per relation type."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_adjacency_matrices(data):\n",
    "    \"\"\"\n",
    "    Converts torch_geometric.datasets.entities data to relation type specific\n",
    "    adjacency matrices\n",
    "    :param data: torch_geometric.datasets.entities data\n",
    "    :return:\n",
    "        A: list of relation type specific adjacency matrices\n",
    "    \"\"\"\n",
    "    num_rels = data.num_rels\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    A = []\n",
    "    source_nodes = data.edge_index[0].numpy()\n",
    "    target_nodes = data.edge_index[1].numpy()\n",
    "\n",
    "    # Get edges for given (relation) edge type and construct adjacency matrix\n",
    "    for i in range(num_rels):\n",
    "        indices = np.argwhere(np.asarray(data.edge_type) == i).squeeze(axis=1)\n",
    "        r_source_nodes = source_nodes[indices]\n",
    "        r_target_nodes = target_nodes[indices]\n",
    "        a = sp.csr_matrix(\n",
    "            (np.ones(len(indices)), (r_source_nodes, r_target_nodes)),\n",
    "            shape=(num_nodes, num_nodes))\n",
    "        A.append(a)\n",
    "\n",
    "    return A"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following functions serve for normalizing the matrices individually and converting them to sparse tensors. The normalization corresponds to the normalization that is performed in a R-GCN layer in order to mitigate the impact of high node degress. In this case, we use the node degree only, as it is suggested in the paper by Schlichtkrull et al. (2018). \\\\\n",
    "We further use sparse tensors because they are memory-efficient. Sparse arrays such as our adjacency matrices, where ones indicate edges, contain a lot of elements equal to zero. Thus, we achieve a more efficient use of processor resources as well as memory if we only store and process the non-zero elements (Torch Contributors, 2019)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def normalize(adj_matrix):\n",
    "    \"\"\"\n",
    "    Normalizes the adjacency matrix\n",
    "    :param adj_matrix: Adjacency matrix\n",
    "    :return:\n",
    "        out: Normalized adjacency matrix\n",
    "    \"\"\"\n",
    "    node_degrees = np.array(adj_matrix.sum(axis=1)).flatten()\n",
    "    # Essentially 1. / node_degrees, while avoiding division by zero warning\n",
    "    norm_const = np.divide(np.ones_like(node_degrees), node_degrees, out=np.zeros_like(node_degrees),\n",
    "                           where=node_degrees != 0)\n",
    "    D_inv = sp.diags(norm_const)\n",
    "    out = D_inv.dot(adj_matrix).tocsr()\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def to_sparse_tensor(sparse_array):\n",
    "    \"\"\"\n",
    "    Converts sparse array (normalized adjacency matrix) to sparse tensor\n",
    "    :param sparse_array: Sparse array (normalized adjacency matrix)\n",
    "    :return:\n",
    "        sparse_tensor: Converted sparse tensor\n",
    "    \"\"\"\n",
    "    if len(sp.find(sparse_array)[-1]) > 0:\n",
    "        # Get indices and values of nonzero elements in matrix\n",
    "        v = torch.FloatTensor(sp.find(sparse_array)[-1])\n",
    "        i = torch.LongTensor(sparse_array.nonzero())\n",
    "        shape = sparse_array.shape\n",
    "        sparse_tensor = torch.sparse_coo_tensor(i, v, torch.Size(shape))\n",
    "    else:\n",
    "        sparse_tensor = torch.sparse_coo_tensor(sparse_array.shape[0], sparse_array.shape[1])\n",
    "    return sparse_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiments\n",
    "In this section, we investigate the accuracy of the model that was implemented within the context of this seminar project. For this purpose, we relied on the same datasets that Schlichtkrull et al. (2018) used in their experiments. In particular, they used the datasets *AM*, *BGS*, *AIFB* and *MUTAG*. These datasets are part of a benchmark for evaluating machine learning approaches in the context of the semantic web (Ristoski, 2016). Relations in those datasets do not necessarily represent the triples of the knowledge graph only but may also provide information on the presence or absence of a feature. The node classification task consists in predicting properties for a specific group of entities for each dataset (Schlichtkrull, 2018). The table below shows the key figures of each dataset in detail."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Description of the Datasets\n",
    "The *AIFB* dataset describes the organizational structure of the AIFB, a research institute at the university *Karlsruher Institut für Technologie*. The dataset includes\n",
    "178 entities representing people that are assigned to a research group within the AIFB. A possible classification task consists in predicting the research group for each entity. In a pre-processing step, the smallest group was omitted as well as the *employs* relation due to it being the inverse of the *affiliate* relation (Ristoski, 2016). In addition to that, the dataset contains information on research papers, external people or projects (Bloehdorn, 2007).\n",
    "\n",
    "The *MUTAG* dataset comprises data on complex, potentially carcinogenic molecules. In this case, the property *isMutagenic* serves as a label (Ristoski, 2016). The dataset represents a set of  molecular graphs in RDF format, where relations encode atomic bonds or the presence of a certain feature (Schlichtkrull, 2018).\n",
    "\n",
    "The *AM* dataset describes artifacts maintained by the Amsterdam Museum, where each artifact refers to metadata on its production, material and content. Each artifact may also be linked to other artifacts and is further labelled with an artifact category, which is used for classification in this case. Ristoski et al. (2016) drew and pre-processed a stratitified random sample of 1000 instances from the complete dataset before including it in the benchmark in its current form. The pre-processing consisted in omitting a relation due to its high correlation with the artifact category.\n",
    "\n",
    "The *BGS* dataset contains geological measurements on so-called named rock units that were collected by the British Geological Survey. Various properties are assigned to these named rock units. In this case, the lithogenesis property is important which divides the rock types into different classes. The rock units belonging to the two largest (in terms of members) lithogenesis classes form the *BGS* dataset as it was used by de Vries et al. (2013) and then adopted by Ristoski et al. (2016) in their proposed benchmark. Thus, the dataset encompasses 146 named\n",
    "rock units, for which the lithogenesis property can be predicted. In the RDF representation of this dataset, relations also either represent the presence of a certain feature or feature hierarchy (Schlichtkrull, 2018).\n",
    "\n",
    "The BGS as well as the MUTAG datasets are special in the sense that high-degree nodes encoding a certain feature act as a hub by linking labeled entities to each other (Schlichtkrull, 2018). These datasets are available in Resource Description Framework (RDF) format. In addition to that, all four datasets are also directly available via `torch.geometric.datasets.Entities`.\n",
    "\n",
    " \\\\\n",
    "\n",
    "\n",
    "| Dataset | AIFB | MUTAG | BGS | AM |\n",
    "| ------- | ---- | ----- | --- | ---|\n",
    "|Entities | 8,285| 23,644|333,845| 1,666,764|\n",
    "|Relations| 45 | 23 | 103 | 133 |\n",
    "|Edges | 29,043 | 74,227 | 916,199 | 5,988,321|\n",
    "|Labeled | 176 | 340 | 146 | 1,000 |\n",
    "|Classes | 4 | 2 | 2 | 11 |\n",
    "\n",
    "<center> <i>Labeled</i> denotes those entities that carry labels. Thus, they are to be classified during the training or testing stage, respectively. <br> Source of the table: (Schlichtkrull, 2018) </center>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the Data\n",
    "In theory, our model should work with all four datasets. However, BGS and AM contain huge graphs, which require lots of memory. That is why we recommend to only use AIFB or MUTAG."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following function is used to load the data via `pytorch geometric`. For that, we make use of the previously defined `get_adjacency_matrices` function to obtain a set of relation type specific adjacency matrices, that are then normalized and converted to sparse tensors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_data(dataset_name):\n",
    "  # Load data via pytorch geometric\n",
    "  path = osp.join('.', 'data', 'Entities')\n",
    "  dataset = Entities(path, dataset_name)\n",
    "  data = dataset[0]\n",
    "\n",
    "  data.num_nodes = maybe_num_nodes(data.edge_index)\n",
    "  data.num_rels = dataset.num_relations\n",
    "\n",
    "  # Construct relation type specific adjacency matrices from data.edge_index and data.edge_type in utils\n",
    "  A = get_adjacency_matrices(data)\n",
    "\n",
    "  adj_t = []\n",
    "  # Normalize matrices individually and convert to sparse tensors\n",
    "  for a in A:\n",
    "      nor_a = normalize(a)\n",
    "      if len(nor_a.nonzero()[0]) > 0:\n",
    "          tensor_a = to_sparse_tensor(nor_a)\n",
    "          adj_t.append(tensor_a.to(device))\n",
    "\n",
    "  # Replace if features are available\n",
    "  x = None\n",
    "  return dataset, data, adj_t, x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment Function\n",
    "For running the experiments, we first create a R-GCN with its parameters set to the given arguments. We further train and evaluate the model for the number of times that is given by `runs`. The optimizer and loss function are initialized each time anew, the parameters are reset in each run as well. We eventually report on the best test accuracy per run as well as the average accuracy with standard error throughout the runs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_experiment(dataset_name, args, runs=10):\n",
    "  dataset, data, adj_t, x = load_data(dataset_name)\n",
    "  # Initialize RGCN model\n",
    "  model = RGCN(\n",
    "      num_nodes=data.num_nodes,\n",
    "      h_dim=args[\"h_dim\"],\n",
    "      out_dim=dataset.num_classes,\n",
    "      num_rels=dataset.num_relations,\n",
    "      num_bases=args[\"num_bases\"],\n",
    "      dropout=args[\"dropout\"]\n",
    "  ).to(device)\n",
    "\n",
    "  test_accs = []\n",
    "\n",
    "  for i in range(1, runs+1):\n",
    "    print('------------------------------------------------')\n",
    "    print(f'Model run {i}')\n",
    "    print('------------------------------------------------')\n",
    "    # Reset the parameters to initial random values\n",
    "    model.reset_parameters()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"], weight_decay=args[\"l2\"])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_test_acc = 0\n",
    "    # Train and evaluate model\n",
    "    for epoch in range(1, args[\"epochs\"] + 1):\n",
    "        loss = train(model, x, adj_t, optimizer, loss_fn, data.train_idx, data.train_y)\n",
    "        train_acc, test_acc = test(model, x, adj_t, data.train_idx, data.train_y, data.test_idx, data.test_y)\n",
    "        if test_acc > best_test_acc:\n",
    "          best_test_acc = test_acc\n",
    "        if epoch == 1 or (epoch % 10) == 0:\n",
    "          print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {train_acc:.4f} '\n",
    "                f'Test: {test_acc:.4f}')\n",
    "    test_accs.append(test_acc)  # alternatively use the best test acc\n",
    "    print(f'Best test accuracy: {best_test_acc:.4f}')\n",
    "\n",
    "  avg_test_acc = np.mean(test_accs)\n",
    "  sem_test_acc = stats.sem(test_accs)\n",
    "\n",
    "  print('------------------------------------------------')\n",
    "  print(f'Average test accuracy over {runs} runs: {100 * avg_test_acc:.2f}+-{100 * sem_test_acc:.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training and Evaluation\n",
    "In our experiments, we train a 2-layer R-GCN with 16 hidden units, i.e., the hidden node representations have a dimension of 16. We run the experiments for 50 epochs with a learning rate of 0.01, while using the Adam optimizer. Adam is an optimization algorithm that adapts the learning rates for each parameter individually. It has been proposed for use in the context of large-scale and high-dimensional machine learning problems (Kingma, 2014).\n",
    "The normalization constant $c_{i,r}$ is set to $|N_{i}^{r}|$, i.e. the average of all incoming messages from a particular relation type. (See the previous function definitions: the normalization constant is realized by splitting the adjacency matrix up into relation-specific adjacency matrices and then normalizing them using the node degrees within each matrix.) We refrained from hyperparameter tuning and adopted the experimental settings that were used by Schlichtkrull et al. (2018)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation on AIFB\n",
    "\n",
    "When evaluating our model on the AIFB dataset, we achieve a test accuracy of 95.28+-0.83 (executed 14.04.2021, 22:56) after 50 epochs of training and averaged over 10 runs with standard error. In comparison to that, Schlichtkrull et al. (2018) report on an accuracy of 95.83+-0.62."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Parameters from the RGCN paper\n",
    "args = {\n",
    "        'h_dim': 16,\n",
    "        'num_bases': -1,  # -1: no basis decomposition, use one weight matrix for each relation\n",
    "        'num_hidden_layers': 0,\n",
    "        'dropout': 0.,\n",
    "        'lr': 0.01,\n",
    "        'l2': 0.,\n",
    "        'bias': False,\n",
    "        'epochs': 50,\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_experiment(dataset_name=\"AIFB\", args=args, runs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation on MUTAG\n",
    "When evaluating our model on the MUTAG dataset, we achieve a test accuracy of 76.91+-1.16 (executed 14.04.2021, 22:56) after 50 epochs of training and averaged over 10 runs with standard error. In comparison to that, Schlichtkrull et al. (2018) report on an accuracy of 73.23+-0.48."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_name = \"MUTAG\"  # choices=['AIFB', 'MUTAG', 'BGS', 'AM']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Parameters from the RGCN paper\n",
    "args = {\n",
    "        'h_dim': 16,\n",
    "        'num_bases': 30,\n",
    "        'num_hidden_layers': 0,\n",
    "        'dropout': 0.,\n",
    "        'lr': 0.01,\n",
    "        'l2': 0.0005,\n",
    "        'bias': False,\n",
    "        'epochs': 50,\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_experiment(dataset_name=\"MUTAG\", args=args, runs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## More Background on (R-)GCNs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Limitations of (R-)GCNs\n",
    "\n",
    "R-GCNs are limited with respect to several aspects (Hamilton, 2020):\n",
    "1. *Limited Support for edge features*: They do not support all kinds of edge features. To be specific, they only support discrete edge features, i.e., edge types.\n",
    "2. *Over-Smoothing*: Several iterations of the GNN message passing procedure may lead to node embeddings becoming very similar to each other, when the information from neighborhood aggregation dominates the update of the node representation. This poses a problem when building deep GNNs that aim at capturing longer-term dependencies.\n",
    "3. *Same normalization constants for all neighbors*: Currently, the same normalization constant is used for each neighbor during the update of a node presentation, so we cannot treat neighbors with varying significance differently.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extensions of (R-)GCNs\n",
    "Several extensions tackle the aforementioned deficiencies. For example, attention-based approaches like Graph Attention Networks (Veličković, 2017) replace normalization constants with learned attention weights, which addresses issue (2) from the previous section. Deep GCNs (Li, 2019) address issue (3) from the previous section by using some tricks to overcome the vanishing gradient problem and thus, enable the exploitation of long-term dependencies."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other Application Areas of (R-)GCNs\n",
    "GNNs are put to use in various application fields: for example in machine translation, where they are used together with dependency graphs to create embeddings enriched with syntactic information (Bastings, 2017). Recommender systems constitute another application area of GNNs, since user-to-item interaction graphs and social graphs can be leveraged. Item embeddings may be learned and used for item-item recommendation, in order to recommend themed collections (e.g. playlists, “feed” content) (Ying, 2018)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "In this seminar project, we presented the mechanics of Graph Convolutional Networks (GCNs) as well as their extension, Relational Graph Convolutional Networks (R-GCNs). Both build upon the same message passing mechanism: in each iteration (layer), every node aggregates information from its local neighborhood in parallel. The more iterations pass, the more information the nodes can gather from further reaches of the graph. The R-GCN introduces the relation-specific aggregation and normalization in order to deal with multi-relational data. We implemented a R-GCN and evaluated it on the datasets AIFB and MUTAG, that were also used in the original paper (Schlichtkrull, 2018). Similar results to those reported in the original paper were achieved."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Resources"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Papers and books:\n",
    "\n",
    "* Bastings, J., Titov, I., Aziz, W., Marcheggiani, D., & Sima'an, K. (2017). Graph convolutional encoders for syntax-aware neural machine translation. *arXiv preprint* arXiv:1704.04675.\n",
    "\n",
    "* Bloehdorn, S., & Sure, Y. (2007). Kernel methods for mining instance data in ontologies. In *The Semantic Web* (pp. 58-71). Springer, Berlin, Heidelberg.\n",
    "\n",
    "* de Vries, G. K. (2013, September). A fast approximation of the Weisfeiler-Lehman graph kernel for RDF data. In *Joint European Conference on Machine Learning and Knowledge Discovery in Databases* (pp. 606-621). Springer, Berlin, Heidelberg.\n",
    "\n",
    "* Dwivedi, V. P., Joshi, C. K., Laurent, T., Bengio, Y., & Bresson, X. (2020). Benchmarking graph neural networks. *arXiv preprint* arXiv:2003.00982.\n",
    "\n",
    "* Hamilton, W. L. (2020). Graph representation learning. *Synthesis Lectures on Artifical Intelligence and Machine Learning*, 14(3), 1-159.\n",
    "\n",
    "* Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. *arXiv preprint* arXiv:1412.6980.\n",
    "\n",
    "* Kipf, T. N., & Welling, M. (2016). Semi-supervised classification with graph convolutional networks. *arXiv preprint* arXiv:1609.02907.\n",
    "\n",
    "* Kipf, T. N. (2020). Deep learning with graph-structured representations.\n",
    "\n",
    "* Li, G., Muller, M., Thabet, A., & Ghanem, B. (2019). Deepgcns: Can gcns go as deep as cnns?. In *Proceedings of the IEEE/CVF International Conference on Computer Vision* (pp. 9267-9276).\n",
    "\n",
    "* Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. *The journal of machine learning research*, 15(1), 1929-1958.\n",
    "\n",
    "* Ristoski, P., De Vries, G. K. D., & Paulheim, H. (2016, October). A collection of benchmark datasets for systematic evaluations of machine learning on the semantic web. In *International Semantic Web Conference* (pp. 186-194). Springer, Cham.\n",
    "\n",
    "* Schlichtkrull, M., Kipf, T. N., Bloem, P., Van Den Berg, R., Titov, I., & Welling, M. (2018, June). Modeling relational data with graph convolutional networks. In *European semantic web conference* (pp. 593-607). Springer, Cham.\n",
    "\n",
    "* Veličković, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., & Bengio, Y. (2017). Graph attention networks. *arXiv preprint* arXiv:1710.10903.\n",
    "\n",
    "* Ying, R., He, R., Chen, K., Eksombatchai, P., Hamilton, W. L., & Leskovec, J. (2018, July). Graph convolutional neural networks for web-scale recommender systems. In *Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining* (pp. 974-983).\n",
    "\n",
    "\n",
    "\n",
    "### Blog Posts:\n",
    "- Dellinger, J. (2019, 03. April). *Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming*. URL https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79\n",
    "- Kipf, T. (2016, 30. September). Graph Convolutional Networks. URL http://tkipf.github.io/graph-convolutional-networks/\n",
    "- Skovgaard Jepsen, T. (2018, 18. September). *How to do Deep Learning on Graphs with Graph Convolutional Networks: Part 1: A High-Level Introduction to Graph Convolutional Networks*. URL https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780\n",
    "- Skovgaard Jepsen, T. (2019, 20. January). *How to do Deep Learning on Graphs with Graph Convolutional Networks: Part 2: Semi-Supervised Learning with Spectral Graph Convolutions*. URL https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-62acf5b143d0\n",
    "\n",
    "### Documentation\n",
    "- Torch Contributors (2019). *Torch.Sparse*. URL https://pytorch.org/docs/stable/sparse.html\n",
    "\n",
    "### Jupyter Notebook Tutorials:\n",
    "- https://github.com/TobiasSkovgaardJepsen/posts/blob/master/HowToDoDeepLearningOnGraphsWithGraphConvolutionalNetworks/Part2_SemiSupervisedLearningWithSpectralGraphConvolutions/notebook.ipynb\n",
    "\n",
    "### Keras Implementation:\n",
    "- https://github.com/tkipf/relational-gcn\n",
    "\n",
    "### PyTorch Implementations\n",
    "- https://github.com/tkipf/pygcn\n",
    "- https://github.com/masakicktashiro/rgcn_pytorch_implementation\n",
    "- https://github.com/mjDelta/relation-gcn-pytorch\n",
    "- https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/4_rgcn.html\n",
    "- https://github.com/rusty1s/pytorch_geometric/blob/master/examples/rgcn.py\n",
    "\n",
    "\n",
    "### Datasets directly available via `torch.geometric.datasets.Entities`: AIFB, MUTAG\n",
    "- Overview: https://www.uni-mannheim.de/dws/research/resources/sw4ml-benchmark/\n",
    "- Download: http://data.dws.informatik.uni-mannheim.de/rmlod/LOD_ML_Datasets/data/datasets/RDF_Datasets/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Relational GCNs\n",
    "Extension of GCNs: Use a set of relation-specific weight matrices $W_r^{(l)}$, where $r \\in R$ denotes the relation type\n",
    "\n",
    "Propagation model:\n",
    "> $h_i^{l+1} = \\sigma\\left(\\sum_{r\\in R}\\sum_{j\\in N^r_i}\\frac{1}{c_{i,r}}W_r^{(l)}h_j^{(l)}+\\underbrace{W_0^{(l)}h_i^{(l)}}_{\\text{self-connection}}\\right)$\n",
    "\n",
    "where \n",
    "- $N^r_i$ denotes the set of neighbor indices of node $i$ under relation $r \\in R$, \n",
    "- $c_{i,r}$ is a problem-specific normalization constant that can either be learned or chosen in advance (such as $c_{i,r} = |N_i^r|$).\n",
    "\n",
    "Neural network layer update: evaluate message passing update in parallel for every node $i \\in V$.\n",
    "\n",
    "Parameter sharing for highly- multi-relational data: basis decomposition of relation-specific weight matrices\n",
    "> $W_r^{(l)} = \\sum^B_{b=1}a^{(l)}_{r,b}V_b^{(l)}$\n",
    "\n",
    "Linear combination of basis transformations $V_b^{(l)} \\in \\mathbb{R}^{d^{(l+1)}\\times d^{(l)}}$ with learnable coefficients $a^{(l)}_{r,b}$ such that only the coefficients depend on $r$. $B$, the number of basis functions, is a hyperparameter.\n",
    "\n",
    "For entity classification as described in the paper minimize:\n",
    "> $L = -\\sum_{i\\in Y}\\sum^K_{k=1}t_{i,k}\\ln h_{i,k}^{(l)}$\n",
    "\n",
    "whre:\n",
    "- $Y$ is the set of node indices with labels\n",
    "- $K$ is the number of classes (?)\n",
    "- $t_{i,k}$ is the ground-truth label\n",
    "- $h_{i,k}^{(l)}$ is the $k$-th entry of network ouput for $i$-th labeled node\n",
    "\n",
    "# Training and evaluation\n",
    "- 2 layer model with 16 hidden units (dimension of hidden node representation)\n",
    "- 50 epochs with learning rate 0.01 using Adam optimizer\n",
    "- normalization constant $c_{i,r} = |N_i^r|$, i.e. average all incoming messages from a particular relation type\n",
    "- $l2$ penalty on first layer weights $C_{l2} \\in \\{0, 5\\cdot 10^{-4}\\}$\n",
    "- number of basis functions $B \\in \\{0, 10, 20, 30, 40\\}$\n",
    "\n",
    "Results reported\n",
    "- Accuracy and standard error over 10 runs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports\n",
    "We mainly use pytorch geometric to load the datasets, numpy and scipy to process the data and pytorch for the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Entities\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy import stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import os.path as osp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### R-GCN layer\n",
    "This part is inspired by the keras implementation from Thomas Kipf and pytorch based implementations:\n",
    "- https://github.com/tkipf/pygcn\n",
    "- https://github.com/masakicktashiro/rgcn_pytorch_implementation\n",
    "- https://github.com/mjDelta/relation-gcn-pytorch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RGCNConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 num_rels,\n",
    "                 num_bases=-1,\n",
    "                 bias=False,\n",
    "                 activation=None,\n",
    "                 dropout=0.5,\n",
    "                 is_output_layer=False):\n",
    "        r\"\"\"The relational graph convolutional operator from the `\"Modeling\n",
    "        Relational Data with Graph Convolutional Networks\"\n",
    "        <https://arxiv.org/abs/1703.06103>`_ paper\n",
    "\n",
    "        Propagation model:\n",
    "        (1) $h_i^{l+1} = \\sigma\\left(\\sum_{r\\in R}\\sum_{j\\in N^r_i}\\frac{1}{c_{i,r}}W_r^{(l)}h_j^{(l)}+\n",
    "        \\underbrace{W_0^{(l)}h_i^{(l)}}_{\\text{self-connection}}\\right)$\n",
    "\n",
    "        where\n",
    "        - $N^r_i$ denotes the set of neighbor indices of node $i$ under relation $r \\in R$,\n",
    "        - $c_{i,r}$ is a problem-specific normalization constant that can either be learned or chosen in advance\n",
    "          (such as $c_{i,r} = |N_i^r|$).\n",
    "\n",
    "        Neural network layer update: evaluate message passing update in parallel for every node $i \\in V$.\n",
    "\n",
    "        Parameter sharing for highly- multi-relational data: basis decomposition of relation-specific weight matrices\n",
    "        (2) $W_r^{(l)} = \\sum^B_{b=1}a^{(l)}_{r,b}V_b^{(l)}$\n",
    "\n",
    "        Linear combination of basis transformations $V_b^{(l)} \\in \\mathbb{R}^{d^{(l+1)}\\times d^{(l)}}$ with learnable\n",
    "        coefficients $a^{(l)}_{r,b}$ such that only the coefficients depend on $r$. $B$, the number of basis functions,\n",
    "        is a hyperparameter.\n",
    "\n",
    "        :param input_dim: Input dimension\n",
    "        :param output_dim: Output dimension\n",
    "        :param num_rels: Number of relation types\n",
    "        :param num_bases: Number of bases used in basis decomposition of relation-specific weight matrices\n",
    "        :param bias: Optional additive bias\n",
    "        :param activation: Activation function\n",
    "        :param dropout: Dropout\n",
    "        :param is_output_layer: Indicates whether this layer is the output layer\n",
    "        \"\"\"\n",
    "        super(RGCNConv, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.is_output_layer = is_output_layer\n",
    "\n",
    "        # Number of bases for the basis decomposition can be less or equal to \n",
    "        # the number of relation types\n",
    "        if self.num_bases <= 0 or self.num_bases > self.num_rels:\n",
    "            self.num_bases = self.num_rels\n",
    "\n",
    "        # Weight bases in equation (2)\n",
    "        # V_b if self.num_bases < self.num_rels, \n",
    "        # W_r if self.num_bases == self.num_rels\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.Tensor(self.num_bases * self.input_dim, self.output_dim))\n",
    "\n",
    "        # Use basis decomposition otherwise if num_bases = num_rels we can just \n",
    "        # use one weight matrix per relation type\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # linear combination coefficients a^{(l)}_{r, b} in equation (2)\n",
    "            self.w_comp = nn.Parameter(\n",
    "                torch.Tensor(self.num_rels, self.num_bases))\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = nn.Parameter(torch.Tensor(self.output_dim))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize trainable parameters, see following link for explanation:\n",
    "        # https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79\n",
    "        # Xavier initialization: improved weight initialization method enabling \n",
    "        # quicker convergence and higher accuracy\n",
    "        # gain is an optional scaling factor, here we use the recommended gain \n",
    "        # value for the given nonlinearity function\n",
    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        if self.num_bases < self.num_rels:\n",
    "            nn.init.xavier_uniform_(self.w_comp, gain=nn.init.calculate_gain('relu'))\n",
    "        if self.bias:\n",
    "            nn.init.xavier_uniform_(self.b, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        supports = []\n",
    "        num_nodes = adj_t[0].shape[0]\n",
    "        for i, adj in enumerate(adj_t):\n",
    "            if x is not None:\n",
    "                supports.append(torch.spmm(adj, x))\n",
    "            else:\n",
    "                supports.append(adj)\n",
    "        supports = torch.cat(supports, dim=1)   # (num_rel, num_nodes*num_rel)\n",
    "\n",
    "        # Calculate relation specific weight matrices\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # Generate all weights from bases as in equation (2)\n",
    "            weight = self.weight.reshape(self.num_bases, self.input_dim, self.output_dim).permute(1, 0, 2)\n",
    "\n",
    "            # Matrix product: learnable coefficients a_{r, b} and basis transformations V_b\n",
    "            # (self.num_rels, self.num_bases) x (self.input_dim, self.num_bases, self.output_dim)\n",
    "            weight = torch.matmul(self.w_comp, weight)  # (self.input_dim, self.num_rels, self.output_dim)\n",
    "            weight = weight.reshape(self.input_dim * self.num_rels, self.output_dim)\n",
    "        else:\n",
    "            weight = self.weight\n",
    "\n",
    "        out = torch.spmm(supports, weight)  # (num_nodes, num_rels)\n",
    "\n",
    "        # If x is None add dropout to output, by elementwise multiplying with \n",
    "        # column vector of ones, with dropout applied to the vector of ones.\n",
    "        if x is None:\n",
    "            temp = torch.ones(num_nodes).to(out.device)\n",
    "            temp_drop = F.dropout(temp, self.dropout)\n",
    "            out = (out.transpose(1, 0) * temp_drop).transpose(1, 0)\n",
    "\n",
    "        if self.bias:\n",
    "            out += self.b\n",
    "\n",
    "        out = self.activation(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### R-GCN model\n",
    "This part is somewhat inspired by: https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/4_rgcn.html.\n",
    "We borrowed the idea of building the model using separate build functions for the input, hidden and output layer in order to allow building models with multiple hidden layers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RGCN(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels,\n",
    "                 num_bases=-1, num_hidden_layers=1, dropout=0.5, bias=False):\n",
    "        \"\"\"\n",
    "        Implementation of R-GCN from the `\"Modeling\n",
    "        Relational Data with Graph Convolutional Networks\"\n",
    "        <https://arxiv.org/abs/1703.06103>`_ paper\n",
    "\n",
    "        :param num_nodes: Number of nodes (input dimension)\n",
    "        :param h_dim: Hidden dimension\n",
    "        :param out_dim: Output dimension\n",
    "        :param num_rels: Number of relation types\n",
    "        :param num_bases: Number of basis functions\n",
    "        :param num_hidden_layers: Number of hidden layers\n",
    "        :param dropout: Dropout probability\n",
    "        :param bias: Whether to use an additive bias\n",
    "        \"\"\"\n",
    "        super(RGCN, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # input to hidden\n",
    "        i2h = self.build_input_layer()\n",
    "        self.layers.append(i2h)\n",
    "        # hidden to hidden\n",
    "        for _ in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer()\n",
    "            self.layers.append(h2h)\n",
    "        # hidden to output\n",
    "        h2o = self.build_output_layer()\n",
    "        self.layers.append(h2o)\n",
    "\n",
    "    def build_input_layer(self):\n",
    "        return RGCNConv(self.num_nodes, self.h_dim, self.num_rels, self.num_bases, activation=F.relu,\n",
    "                        dropout=self.dropout, bias=self.bias)\n",
    "\n",
    "    def build_hidden_layer(self):\n",
    "        return RGCNConv(self.h_dim, self.h_dim, self.num_rels, self.num_bases, activation=F.relu,\n",
    "                        dropout=self.dropout, bias=self.bias)\n",
    "\n",
    "    def build_output_layer(self):\n",
    "        return RGCNConv(self.h_dim, self.out_dim, self.num_rels, self.num_bases, activation=partial(F.softmax, dim=-1),\n",
    "                        dropout=self.dropout, is_output_layer=True, bias=self.bias)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.layers:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, adj_t)\n",
    "            if not layer.is_output_layer:\n",
    "                out = F.dropout(out, self.dropout, self.training)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training and evaluation functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model, x, adj_t, optimizer, loss_fn, train_idx, train_y):\n",
    "    model.train()\n",
    "\n",
    "    # Zero grad the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    # Feed the data into the model\n",
    "    out = model(x, adj_t)\n",
    "    # Feed the sliced output and label to loss_fn\n",
    "    labels = torch.LongTensor(train_y).to(out.device)\n",
    "    loss = loss_fn(out[train_idx], labels)\n",
    "\n",
    "    # Backpropagation, optimizer\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, x, adj_t, train_idx, train_y, test_idx, test_y):\n",
    "    model.eval()\n",
    "\n",
    "    # Output of model on all data\n",
    "    out = model(x, adj_t)\n",
    "    # Get predicted class labels\n",
    "    pred = out.argmax(dim=-1).cpu()\n",
    "\n",
    "    # Evaluate prediction accuracy\n",
    "    train_acc = pred[train_idx].eq(train_y).to(torch.float).mean()\n",
    "    test_acc = pred[test_idx].eq(test_y).to(torch.float).mean()\n",
    "    return train_acc.item(), test_acc.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data preprocessing functions\n",
    "In order to use the data from `torch.geometric.datasets.Entities` with our R-GCN implementation we have to convert the dataset and construct the adjacency matrices from the edge index and edge type arrays."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_adjacency_matrices(data):\n",
    "    \"\"\"\n",
    "    Converts torch_geometric.datasets.entities data to relation type specific \n",
    "    adjacency matrices\n",
    "    :param data: torch_geometric.datasets.entities data\n",
    "    :return:\n",
    "        A: list of relation type specific adjacency matrices\n",
    "    \"\"\"\n",
    "    num_rels = data.num_rels\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    A = []\n",
    "    source_nodes = data.edge_index[0].numpy()\n",
    "    target_nodes = data.edge_index[1].numpy()\n",
    "\n",
    "    # Get edges for given (relation) edge type and construct adjacency matrix\n",
    "    for i in range(num_rels):\n",
    "        indices = np.argwhere(np.asarray(data.edge_type) == i).squeeze(axis=1)\n",
    "        r_source_nodes = source_nodes[indices]\n",
    "        r_target_nodes = target_nodes[indices]\n",
    "        a = sp.csr_matrix(\n",
    "            (np.ones(len(indices)), (r_source_nodes, r_target_nodes)), \n",
    "            shape=(num_nodes, num_nodes))\n",
    "        A.append(a)\n",
    "\n",
    "    return A"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following functions are for normalizing the matrices individually and converting them to sparse tensors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def normalize(adj_matrix):\n",
    "    \"\"\"\n",
    "    Normalizes the adjacency matrix\n",
    "    :param adj_matrix: Adjacency matrix\n",
    "    :return:\n",
    "        out: Normalized adjacency matrix\n",
    "    \"\"\"\n",
    "    node_degrees = np.array(adj_matrix.sum(axis=1)).flatten()\n",
    "    # Essentially 1. / node_degrees, while avoiding division by zero warning\n",
    "    norm_const = np.divide(np.ones_like(node_degrees), node_degrees, out=np.zeros_like(node_degrees),\n",
    "                           where=node_degrees != 0)\n",
    "    D_inv = sp.diags(norm_const)\n",
    "    out = D_inv.dot(adj_matrix).tocsr()\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def to_sparse_tensor(sparse_array):\n",
    "    \"\"\"\n",
    "    Converts sparse array (normalized adjacency matrix) to sparse tensor\n",
    "    :param sparse_array: Sparse array (normalized adjacency matrix)\n",
    "    :return:\n",
    "        sparse_tensor: Converted sparse tensor\n",
    "    \"\"\"\n",
    "    if len(sp.find(sparse_array)[-1]) > 0:\n",
    "        # Get indices and values of nonzero elements in matrix\n",
    "        v = torch.FloatTensor(sp.find(sparse_array)[-1])\n",
    "        i = torch.LongTensor(sparse_array.nonzero())\n",
    "        shape = sparse_array.shape\n",
    "        sparse_tensor = torch.sparse_coo_tensor(i, v, torch.Size(shape))\n",
    "    else:\n",
    "        sparse_tensor = torch.sparse_coo_tensor(sparse_array.shape[0], sparse_array.shape[1])\n",
    "    return sparse_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiments\n",
    "In theory this model should work with all 4 datasets. However BGS and AM contain huge graphs, which require lots of memory. We recommend to only use AIFB or MUTAG."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data loading function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_data(dataset_name):\n",
    "  # Load data via pytorch geometric\n",
    "  path = osp.join('.', 'data', 'Entities')\n",
    "  dataset = Entities(path, dataset_name)\n",
    "  data = dataset[0]\n",
    "\n",
    "  data.num_nodes = maybe_num_nodes(data.edge_index)\n",
    "  data.num_rels = dataset.num_relations\n",
    "\n",
    "  # Construct relation type specific adjacency matrices from data.edge_index and data.edge_type in utils\n",
    "  A = get_adjacency_matrices(data)\n",
    "\n",
    "  adj_t = []\n",
    "  # Normalize matrices individually and convert to sparse tensors\n",
    "  for a in A:\n",
    "      nor_a = normalize(a)\n",
    "      if len(nor_a.nonzero()[0]) > 0:\n",
    "          tensor_a = to_sparse_tensor(nor_a)\n",
    "          adj_t.append(tensor_a.to(device))\n",
    "\n",
    "  # Replace if features are available\n",
    "  x = None    \n",
    "  return dataset, data, adj_t, x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Experiment function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_experiment(dataset_name, args, runs=10):\n",
    "  dataset, data, adj_t, x = load_data(dataset_name)\n",
    "  # Initialize RGCN model\n",
    "  model = RGCN(\n",
    "      num_nodes=data.num_nodes,\n",
    "      h_dim=args[\"h_dim\"],\n",
    "      out_dim=dataset.num_classes,\n",
    "      num_rels=dataset.num_relations,\n",
    "      num_bases=args[\"num_bases\"],\n",
    "      dropout=args[\"dropout\"]\n",
    "  ).to(device)\n",
    "\n",
    "  test_accs = []\n",
    "\n",
    "  for i in range(1, runs+1):\n",
    "    print('------------------------------------------------')\n",
    "    print(f'Model run {i}')\n",
    "    print('------------------------------------------------')\n",
    "    # Reset the parameters to initial random values\n",
    "    model.reset_parameters()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"], weight_decay=args[\"l2\"])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_test_acc = 0\n",
    "    test_acc = 0\n",
    "    # Train and evaluate model\n",
    "    for epoch in range(1, args[\"epochs\"] + 1):\n",
    "        loss = train(model, x, adj_t, optimizer, loss_fn, data.train_idx, data.train_y)\n",
    "        train_acc, test_acc = test(model, x, adj_t, data.train_idx, data.train_y, data.test_idx, data.test_y)\n",
    "        if test_acc > best_test_acc:\n",
    "          best_test_acc = test_acc\n",
    "        if epoch == 1 or (epoch % 10) == 0:\n",
    "          print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {train_acc:.4f} '\n",
    "                f'Test: {test_acc:.4f}')\n",
    "    test_accs.append(test_acc)  # alternatively use the best test acc\n",
    "    print(f'Best test accuracy: {best_test_acc:.4f}')\n",
    "  \n",
    "  avg_test_acc = np.mean(test_accs)\n",
    "  sem_test_acc = stats.sem(test_accs)\n",
    "\n",
    "  print('------------------------------------------------')\n",
    "  print(f'Average test accuracy over {runs} runs: {100 * avg_test_acc:.2f}+-{100 * sem_test_acc:.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### AIFB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Parameters from the RGCN paper\n",
    "args = {\n",
    "        'h_dim': 16,\n",
    "        'num_bases': -1,\n",
    "        'num_hidden_layers': 0,\n",
    "        'dropout': 0.,\n",
    "        'lr': 0.01,\n",
    "        'l2': 0.,\n",
    "        'bias': False,\n",
    "        'epochs': 50,\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training and Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_experiment(dataset_name=\"AIFB\", args=args, runs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MUTAG"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_name = \"MUTAG\"  # choices=['AIFB', 'MUTAG', 'BGS', 'AM']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Parameters from the RGCN paper\n",
    "args = {\n",
    "        'h_dim': 16,\n",
    "        'num_bases': 30,\n",
    "        'num_hidden_layers': 0,\n",
    "        'dropout': 0.,\n",
    "        'lr': 0.01,\n",
    "        'l2': 0.0005,\n",
    "        'bias': False,\n",
    "        'epochs': 50,\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training and Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_experiment(dataset_name=\"MUTAG\", args=args, runs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NDinDL_R-GCNs.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}